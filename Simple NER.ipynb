{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88f989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdbe247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 14:27:47.939517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-17 14:27:48.077418: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:48.077447: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-17 14:27:48.109461: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-17 14:27:49.198381: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:49.198457: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:49.198464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-17 14:27:52.812045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:52.812125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:52.812166: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:52.812258: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:52.812295: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:52.812333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:27:52.812340: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from random import Random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed)\n",
    "import numpy as np\n",
    "import datasets\n",
    "from collections import defaultdict\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a916e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0590b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 32\n",
    "num_train_epochs = 5\n",
    "weight_decay = 0.1\n",
    "warmup_ratio = 0.1\n",
    "learning_rate = 5e-5\n",
    "load_best_model_at_end = True\n",
    "output_dir = \"/mounts/work/akoksal/earthquake_ner_models/\"\n",
    "data_path = \"annotated_address_dataset_07022023_766train_192test/\"\n",
    "cache_dir = \"/mounts/work/akoksal/hf_cache\"\n",
    "saved_models_path = \"/mounts/work/akoksal/earthquake_ner_models/\"\n",
    "device = \"cuda\"\n",
    "seed = 42\n",
    "model_names = [\"dbmdz/bert-base-turkish-cased\",\n",
    "              \"dbmdz/electra-base-turkish-mc4-cased-discriminator\",\n",
    "              \"dbmdz/bert-base-turkish-128k-cased\",\n",
    "              \"dbmdz/convbert-base-turkish-cased\",\n",
    "              \"bert-base-multilingual-cased\",\n",
    "              \"xlm-roberta-base\"]\n",
    "model_name = model_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aeb3dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbmdz/bert-base-turkish-cased'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffeb73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a876c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-no\",\n",
    "    2: \"I-no\",\n",
    "    3: \"B-mahalle\",\n",
    "    4: \"I-mahalle\",\n",
    "    5: \"B-ad-soyad\",\n",
    "    6: \"I-ad-soyad\",\n",
    "    7: \"B-kat\",\n",
    "    8: \"I-kat\",\n",
    "    9: \"B-ilce\",\n",
    "    10: \"I-ilce\",\n",
    "    11: \"B-sokak\",\n",
    "    12: \"I-sokak\",\n",
    "    13: \"B-dis kapi no\",\n",
    "    14: \"I-dis kapi no\",\n",
    "    15: \"B-Apartman/Site\",\n",
    "    16: \"I-Apartman/Site\",\n",
    "    17: \"B-ic kapi no\",\n",
    "    18: \"I-ic kapi no\",\n",
    "    19: \"B-il\",\n",
    "    20: \"I-il\"\n",
    "}\n",
    "\n",
    "label2id = {label: idx for idx, label in id2label.items()}\n",
    "label_names = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4540c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                        num_labels=len(label_names),\n",
    "                                                        id2label=id2label,\n",
    "                                                        cache_dir=cache_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a66af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /mounts/Users/cisintern/akoksal/Earthquake NER/annotated_address_dataset_07022023_766train_192test/train/cache-3e1964df5024fe8f.arrow\n",
      "Loading cached processed dataset at /mounts/Users/cisintern/akoksal/Earthquake NER/annotated_address_dataset_07022023_766train_192test/test/cache-20089035361419de.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk(data_path)\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "    tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True)\n",
    "    #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "    #so the new keys [input_ids, labels (after adjustment)]\n",
    "    #can be added to the datasets dict for each train test validation split\n",
    "    total_adjusted_labels = []\n",
    "    print(len(tokenized_samples[\"input_ids\"]))\n",
    "    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "        prev_wid = -1\n",
    "        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "        existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "        i = -1\n",
    "        adjusted_label_ids = []\n",
    "\n",
    "        for wid in word_ids_list:\n",
    "            if(wid is None):\n",
    "                adjusted_label_ids.append(-100)\n",
    "            elif(wid!=prev_wid):\n",
    "                i = i + 1\n",
    "                adjusted_label_ids.append(existing_label_ids[i])\n",
    "                prev_wid = wid\n",
    "            else:\n",
    "                label_name = label_names[existing_label_ids[i]]\n",
    "                adjusted_label_ids.append(existing_label_ids[i])\n",
    "\n",
    "        total_adjusted_labels.append(adjusted_label_ids)\n",
    "    tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "    return tokenized_samples\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b43934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24f52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3781608/885599324.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "            flattened_results[k+\"_recall\"]=results[k][\"recall\"]\n",
    "            flattened_results[k+\"_precision\"]=results[k][\"precision\"]\n",
    "            flattened_results[k+\"_support\"]=results[k][\"number\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a955fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=saved_models_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=weight_decay,\n",
    "    run_name = \"turkish_ner\",\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=load_best_model_at_end\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f78efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 766\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 110042901\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Apartman/site F1</th>\n",
       "      <th>Apartman/site Recall</th>\n",
       "      <th>Apartman/site Precision</th>\n",
       "      <th>Apartman/site Support</th>\n",
       "      <th>Ad-soyad F1</th>\n",
       "      <th>Ad-soyad Recall</th>\n",
       "      <th>Ad-soyad Precision</th>\n",
       "      <th>Ad-soyad Support</th>\n",
       "      <th>Dis kapi no F1</th>\n",
       "      <th>Dis kapi no Recall</th>\n",
       "      <th>Dis kapi no Precision</th>\n",
       "      <th>Dis kapi no Support</th>\n",
       "      <th>Ic kapi no F1</th>\n",
       "      <th>Ic kapi no Recall</th>\n",
       "      <th>Ic kapi no Precision</th>\n",
       "      <th>Ic kapi no Support</th>\n",
       "      <th>Il F1</th>\n",
       "      <th>Il Recall</th>\n",
       "      <th>Il Precision</th>\n",
       "      <th>Il Support</th>\n",
       "      <th>Ilce F1</th>\n",
       "      <th>Ilce Recall</th>\n",
       "      <th>Ilce Precision</th>\n",
       "      <th>Ilce Support</th>\n",
       "      <th>Kat F1</th>\n",
       "      <th>Kat Recall</th>\n",
       "      <th>Kat Precision</th>\n",
       "      <th>Kat Support</th>\n",
       "      <th>Mahalle F1</th>\n",
       "      <th>Mahalle Recall</th>\n",
       "      <th>Mahalle Precision</th>\n",
       "      <th>Mahalle Support</th>\n",
       "      <th>No F1</th>\n",
       "      <th>No Recall</th>\n",
       "      <th>No Precision</th>\n",
       "      <th>No Support</th>\n",
       "      <th>Sokak F1</th>\n",
       "      <th>Sokak Recall</th>\n",
       "      <th>Sokak Precision</th>\n",
       "      <th>Sokak Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.253700</td>\n",
       "      <td>0.527912</td>\n",
       "      <td>0.328418</td>\n",
       "      <td>0.390127</td>\n",
       "      <td>0.356623</td>\n",
       "      <td>0.839632</td>\n",
       "      <td>0.302013</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>136</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.262948</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467949</td>\n",
       "      <td>0.388298</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>188</td>\n",
       "      <td>0.415789</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.349558</td>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>0.449524</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.367601</td>\n",
       "      <td>204</td>\n",
       "      <td>0.332130</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.228856</td>\n",
       "      <td>76</td>\n",
       "      <td>0.279379</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.450677</td>\n",
       "      <td>0.393959</td>\n",
       "      <td>0.488057</td>\n",
       "      <td>0.435989</td>\n",
       "      <td>0.857910</td>\n",
       "      <td>0.385382</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>136</td>\n",
       "      <td>0.321755</td>\n",
       "      <td>0.350598</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.607407</td>\n",
       "      <td>0.654255</td>\n",
       "      <td>0.566820</td>\n",
       "      <td>188</td>\n",
       "      <td>0.498498</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.463687</td>\n",
       "      <td>154</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>23</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>204</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.276074</td>\n",
       "      <td>76</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.433673</td>\n",
       "      <td>0.348361</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.420699</td>\n",
       "      <td>0.453655</td>\n",
       "      <td>0.479299</td>\n",
       "      <td>0.466125</td>\n",
       "      <td>0.866791</td>\n",
       "      <td>0.429066</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.405229</td>\n",
       "      <td>136</td>\n",
       "      <td>0.372951</td>\n",
       "      <td>0.362550</td>\n",
       "      <td>0.383966</td>\n",
       "      <td>251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.673684</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>188</td>\n",
       "      <td>0.520124</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.497041</td>\n",
       "      <td>154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>23</td>\n",
       "      <td>0.565702</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.518367</td>\n",
       "      <td>204</td>\n",
       "      <td>0.374332</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>76</td>\n",
       "      <td>0.342711</td>\n",
       "      <td>0.341837</td>\n",
       "      <td>0.343590</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.408484</td>\n",
       "      <td>0.456745</td>\n",
       "      <td>0.496019</td>\n",
       "      <td>0.475573</td>\n",
       "      <td>0.870405</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>136</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>0.390438</td>\n",
       "      <td>0.362963</td>\n",
       "      <td>251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.668508</td>\n",
       "      <td>0.643617</td>\n",
       "      <td>0.695402</td>\n",
       "      <td>188</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.543210</td>\n",
       "      <td>154</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>23</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.521186</td>\n",
       "      <td>204</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>76</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.417334</td>\n",
       "      <td>0.465976</td>\n",
       "      <td>0.501592</td>\n",
       "      <td>0.483129</td>\n",
       "      <td>0.872264</td>\n",
       "      <td>0.408304</td>\n",
       "      <td>0.433824</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>136</td>\n",
       "      <td>0.390342</td>\n",
       "      <td>0.386454</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.672043</td>\n",
       "      <td>0.664894</td>\n",
       "      <td>0.679348</td>\n",
       "      <td>188</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>23</td>\n",
       "      <td>0.574032</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.536170</td>\n",
       "      <td>204</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.314516</td>\n",
       "      <td>76</td>\n",
       "      <td>0.429293</td>\n",
       "      <td>0.433673</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-48\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-144] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-96\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-192] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-144\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-240] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-192\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-48] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-240\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-96] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /mounts/work/akoksal/earthquake_ner_models/checkpoint-192 (score: 0.4084841310977936).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=0.5215797344843547, metrics={'train_runtime': 45.8302, 'train_samples_per_second': 83.569, 'train_steps_per_second': 5.237, 'total_flos': 178119641824452.0, 'train_loss': 0.5215797344843547, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4427c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c87f6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>1256</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apartman/Site</th>\n",
       "      <td>136</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad-soyad</th>\n",
       "      <td>251</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis kapi no</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ic kapi no</th>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il</th>\n",
       "      <td>188</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ilce</th>\n",
       "      <td>154</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kat</th>\n",
       "      <td>23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mahalle</th>\n",
       "      <td>204</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>76</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sokak</th>\n",
       "      <td>196</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               support  precision  recall    f1  accuracy\n",
       "overall           1256       0.46    0.50  0.48      0.87\n",
       "Apartman/Site      136       0.41    0.38  0.39       NaN\n",
       "ad-soyad           251       0.36    0.39  0.38       NaN\n",
       "dis kapi no         16       0.00    0.00  0.00       NaN\n",
       "ic kapi no          12       0.00    0.00  0.00       NaN\n",
       "il                 188       0.70    0.64  0.67       NaN\n",
       "ilce               154       0.54    0.57  0.56       NaN\n",
       "kat                 23       0.26    0.48  0.34       NaN\n",
       "mahalle            204       0.52    0.60  0.56       NaN\n",
       "no                  76       0.35    0.51  0.41       NaN\n",
       "sokak              196       0.38    0.46  0.42       NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_results = defaultdict(dict)\n",
    "structured_results[\"overall\"][\"support\"]=0\n",
    "for x, y in results.items():\n",
    "    if len(x.split(\"_\"))==3:\n",
    "        structured_results[x.split(\"_\")[1]][x.split(\"_\")[2]] = y\n",
    "        if x.split(\"_\")[2]==\"support\":\n",
    "            structured_results[\"overall\"][\"support\"]+=y\n",
    "results_pd = pd.DataFrame(structured_results).T\n",
    "results_pd.support = results_pd.support.astype(int)\n",
    "results_pd.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3de283",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed165edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"ner\", model=model.to(device), tokenizer=tokenizer, aggregation_strategy=\"first\", device=0 if device==\"cuda\" else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e350503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.thepythoncode.com/article/named-entity-recognition-using-transformers-and-spacy\n",
    "def get_entities_html(text, ner_result, title=None):\n",
    "    \"\"\"Visualize NER with the help of SpaCy\"\"\"\n",
    "    ents = []\n",
    "    for ent in ner_result:\n",
    "        e = {}\n",
    "        # add the start and end positions of the entity\n",
    "        e[\"start\"] = ent[\"start\"]\n",
    "        e[\"end\"] = ent[\"end\"]\n",
    "        # add the score if you want in the label\n",
    "        # e[\"label\"] = f\"{ent[\"entity\"]}-{ent['score']:.2f}\"\n",
    "        e[\"label\"] = ent[\"entity_group\"]\n",
    "        if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "            # if the current entity is shared with previous entity\n",
    "            # simply extend the entity end position instead of adding a new one\n",
    "            ents[-1][\"end\"] = e[\"end\"]\n",
    "            continue\n",
    "        ents.append(e)\n",
    "      # construct data required for displacy.render() method\n",
    "    render_data = [\n",
    "    {\n",
    "      \"text\": text,\n",
    "      \"ents\": ents,\n",
    "      \"title\": title,\n",
    "    }\n",
    "    ]\n",
    "    spacy.displacy.render(render_data, style=\"ent\", manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98a6902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Lütfen yardım \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Akevler mahallesi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">mahalle</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rüzgar sokak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">sokak</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuncay apartmanı\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Apartman/Site</span>\n",
       "</mark>\n",
       " zemin kat \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Antakya\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ilce</span>\n",
       "</mark>\n",
       " akrabalarım göçük altında #hatay #Afad</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"\"\"Lütfen yardım Akevler mahallesi Rüzgar sokak Tuncay apartmanı zemin kat Antakya akrabalarım göçük altında #hatay #Afad\"\"\"\n",
    "\n",
    "get_entities_html(sentence, nlp(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b823ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">@Cihatzn05 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fethiye mahallesi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">mahalle</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    geni sokak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">sokak</span>\n",
       "</mark>\n",
       "   \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Adıyaman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">il</span>\n",
       "</mark>\n",
       " / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tut 12\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ilce</span>\n",
       "</mark>\n",
       " den beri ulaşamıyorum</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \" \".join(dataset[\"train\"][433][\"tokens\"])\n",
    "get_entities_html(sentence, nlp(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b233a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
