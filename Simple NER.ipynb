{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88f989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdbe247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 21:50:14.807003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 21:50:14.940255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:14.940283: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-19 21:50:14.971344: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-19 21:50:15.785053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:15.785118: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:15.785124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-19 21:50:18.106197: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:18.106288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:18.106340: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:18.106451: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:18.106500: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:18.106551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-19 21:50:18.106560: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from random import Random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed)\n",
    "import numpy as np\n",
    "import datasets\n",
    "from collections import defaultdict\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a916e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0590b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 32\n",
    "num_train_epochs = 5\n",
    "weight_decay = 0.1\n",
    "warmup_ratio = 0.1\n",
    "learning_rate = 5e-5\n",
    "load_best_model_at_end = True\n",
    "output_dir = \"/mounts/work/akoksal/earthquake_ner_models/\"\n",
    "data_path = \"annotated_address_dataset_07022023_766train_192test/\"\n",
    "cache_dir = \"/mounts/work/akoksal/hf_cache\"\n",
    "saved_models_path = \"/mounts/work/akoksal/earthquake_ner_models/\"\n",
    "device = \"cuda\"\n",
    "seed = 42\n",
    "model_names = [\"dbmdz/bert-base-turkish-cased\",\n",
    "              \"dbmdz/electra-base-turkish-mc4-cased-discriminator\",\n",
    "              \"dbmdz/bert-base-turkish-128k-cased\",\n",
    "              \"dbmdz/convbert-base-turkish-cased\",\n",
    "              \"bert-base-multilingual-cased\",\n",
    "              \"xlm-roberta-base\"]\n",
    "model_name = model_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aeb3dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbmdz/bert-base-turkish-cased'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffeb73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a876c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-no\",\n",
    "    2: \"I-no\",\n",
    "    3: \"B-mahalle\",\n",
    "    4: \"I-mahalle\",\n",
    "    5: \"B-ad-soyad\",\n",
    "    6: \"I-ad-soyad\",\n",
    "    7: \"B-kat\",\n",
    "    8: \"I-kat\",\n",
    "    9: \"B-ilce\",\n",
    "    10: \"I-ilce\",\n",
    "    11: \"B-sokak\",\n",
    "    12: \"I-sokak\",\n",
    "    13: \"B-dis kapi no\",\n",
    "    14: \"I-dis kapi no\",\n",
    "    15: \"B-Apartman/Site\",\n",
    "    16: \"I-Apartman/Site\",\n",
    "    17: \"B-ic kapi no\",\n",
    "    18: \"I-ic kapi no\",\n",
    "    19: \"B-il\",\n",
    "    20: \"I-il\"\n",
    "}\n",
    "\n",
    "label2id = {label: idx for idx, label in id2label.items()}\n",
    "label_names = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4540c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                        num_labels=len(label_names),\n",
    "                                                        id2label=id2label,\n",
    "                                                        cache_dir=cache_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a66af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd0653cf4904f0bb77c3205c0980f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41d09ec4b9441095e8e3e9bc940429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk(data_path)\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b43934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24f52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3903590/885599324.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "            flattened_results[k+\"_recall\"]=results[k][\"recall\"]\n",
    "            flattened_results[k+\"_precision\"]=results[k][\"precision\"]\n",
    "            flattened_results[k+\"_support\"]=results[k][\"number\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a955fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=saved_models_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=weight_decay,\n",
    "    run_name = \"turkish_ner\",\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=load_best_model_at_end\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f78efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 766\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 110042901\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Apartman/site F1</th>\n",
       "      <th>Apartman/site Recall</th>\n",
       "      <th>Apartman/site Precision</th>\n",
       "      <th>Apartman/site Support</th>\n",
       "      <th>Ad-soyad F1</th>\n",
       "      <th>Ad-soyad Recall</th>\n",
       "      <th>Ad-soyad Precision</th>\n",
       "      <th>Ad-soyad Support</th>\n",
       "      <th>Dis kapi no F1</th>\n",
       "      <th>Dis kapi no Recall</th>\n",
       "      <th>Dis kapi no Precision</th>\n",
       "      <th>Dis kapi no Support</th>\n",
       "      <th>Ic kapi no F1</th>\n",
       "      <th>Ic kapi no Recall</th>\n",
       "      <th>Ic kapi no Precision</th>\n",
       "      <th>Ic kapi no Support</th>\n",
       "      <th>Il F1</th>\n",
       "      <th>Il Recall</th>\n",
       "      <th>Il Precision</th>\n",
       "      <th>Il Support</th>\n",
       "      <th>Ilce F1</th>\n",
       "      <th>Ilce Recall</th>\n",
       "      <th>Ilce Precision</th>\n",
       "      <th>Ilce Support</th>\n",
       "      <th>Kat F1</th>\n",
       "      <th>Kat Recall</th>\n",
       "      <th>Kat Precision</th>\n",
       "      <th>Kat Support</th>\n",
       "      <th>Mahalle F1</th>\n",
       "      <th>Mahalle Recall</th>\n",
       "      <th>Mahalle Precision</th>\n",
       "      <th>Mahalle Support</th>\n",
       "      <th>No F1</th>\n",
       "      <th>No Recall</th>\n",
       "      <th>No Precision</th>\n",
       "      <th>No Support</th>\n",
       "      <th>Sokak F1</th>\n",
       "      <th>Sokak Recall</th>\n",
       "      <th>Sokak Precision</th>\n",
       "      <th>Sokak Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.187100</td>\n",
       "      <td>0.342450</td>\n",
       "      <td>0.530374</td>\n",
       "      <td>0.663743</td>\n",
       "      <td>0.589610</td>\n",
       "      <td>0.903123</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>75</td>\n",
       "      <td>0.765799</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>0.700680</td>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>106</td>\n",
       "      <td>0.573248</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>112</td>\n",
       "      <td>0.335570</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.233645</td>\n",
       "      <td>42</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.476510</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.285237</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.687332</td>\n",
       "      <td>0.915457</td>\n",
       "      <td>0.629834</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.537736</td>\n",
       "      <td>75</td>\n",
       "      <td>0.787645</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.744526</td>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.862222</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>0.815126</td>\n",
       "      <td>106</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>84</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.630252</td>\n",
       "      <td>0.669643</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>112</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>42</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.604839</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.257390</td>\n",
       "      <td>0.661125</td>\n",
       "      <td>0.755848</td>\n",
       "      <td>0.705321</td>\n",
       "      <td>0.924209</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>75</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>0.759398</td>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>106</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>84</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>13</td>\n",
       "      <td>0.629787</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>112</td>\n",
       "      <td>0.376068</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>42</td>\n",
       "      <td>0.697872</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.252394</td>\n",
       "      <td>0.675497</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.708826</td>\n",
       "      <td>0.926795</td>\n",
       "      <td>0.654088</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>75</td>\n",
       "      <td>0.809339</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "      <td>0.919431</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>106</td>\n",
       "      <td>0.828729</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>84</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>112</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>42</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.264283</td>\n",
       "      <td>0.687166</td>\n",
       "      <td>0.751462</td>\n",
       "      <td>0.717877</td>\n",
       "      <td>0.927392</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>75</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>0.786260</td>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>106</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>84</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>13</td>\n",
       "      <td>0.632479</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>112</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>42</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-48\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-144] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-96\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-192] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-144\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-240] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-192\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-48] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /mounts/work/akoksal/earthquake_ner_models/checkpoint-240\n",
      "Configuration saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/config.json\n",
      "Model weights saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in /mounts/work/akoksal/earthquake_ner_models/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [/mounts/work/akoksal/earthquake_ner_models/checkpoint-96] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /mounts/work/akoksal/earthquake_ner_models/checkpoint-192 (score: 0.252394437789917).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=0.3705816388130188, metrics={'train_runtime': 45.7151, 'train_samples_per_second': 83.78, 'train_steps_per_second': 5.25, 'total_flos': 178119641824452.0, 'train_loss': 0.3705816388130188, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4427c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tokens, ner_tags. If id, tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 192\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/akoksal/anaconda3/envs/lmbias/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922a7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>684</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apartman/Site</th>\n",
       "      <td>75</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad-soyad</th>\n",
       "      <td>122</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis kapi no</th>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ic kapi no</th>\n",
       "      <td>10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il</th>\n",
       "      <td>106</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ilce</th>\n",
       "      <td>84</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kat</th>\n",
       "      <td>13</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mahalle</th>\n",
       "      <td>112</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sokak</th>\n",
       "      <td>111</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               support  precision  recall    f1  accuracy\n",
       "overall            684       0.68    0.75  0.71      0.93\n",
       "Apartman/Site       75       0.62    0.69  0.65       NaN\n",
       "ad-soyad           122       0.77    0.85  0.81       NaN\n",
       "dis kapi no          9       0.00    0.00  0.00       NaN\n",
       "ic kapi no          10       0.17    0.20  0.18       NaN\n",
       "il                 106       0.92    0.92  0.92       NaN\n",
       "ilce                84       0.77    0.89  0.83       NaN\n",
       "kat                 13       0.35    0.54  0.42       NaN\n",
       "mahalle            112       0.65    0.69  0.67       NaN\n",
       "no                  42       0.38    0.57  0.46       NaN\n",
       "sokak              111       0.60    0.65  0.62       NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_results = defaultdict(dict)\n",
    "structured_results[\"overall\"][\"support\"]=0\n",
    "for x, y in results.items():\n",
    "    if len(x.split(\"_\"))==3:\n",
    "        structured_results[x.split(\"_\")[1]][x.split(\"_\")[2]] = y\n",
    "        if x.split(\"_\")[2]==\"support\":\n",
    "            structured_results[\"overall\"][\"support\"]+=y\n",
    "results_pd = pd.DataFrame(structured_results).T\n",
    "results_pd.support = results_pd.support.astype(int)\n",
    "results_pd.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3de283",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed165edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"ner\", model=model.to(device), tokenizer=tokenizer, aggregation_strategy=\"first\", device=0 if device==\"cuda\" else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e350503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.thepythoncode.com/article/named-entity-recognition-using-transformers-and-spacy\n",
    "def get_entities_html(text, ner_result, title=None):\n",
    "    \"\"\"Visualize NER with the help of SpaCy\"\"\"\n",
    "    ents = []\n",
    "    for ent in ner_result:\n",
    "        e = {}\n",
    "        # add the start and end positions of the entity\n",
    "        e[\"start\"] = ent[\"start\"]\n",
    "        e[\"end\"] = ent[\"end\"]\n",
    "        # add the score if you want in the label\n",
    "        # e[\"label\"] = f\"{ent[\"entity\"]}-{ent['score']:.2f}\"\n",
    "        e[\"label\"] = ent[\"entity_group\"]\n",
    "        if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
    "            # if the current entity is shared with previous entity\n",
    "            # simply extend the entity end position instead of adding a new one\n",
    "            ents[-1][\"end\"] = e[\"end\"]\n",
    "            continue\n",
    "        ents.append(e)\n",
    "      # construct data required for displacy.render() method\n",
    "    render_data = [\n",
    "    {\n",
    "      \"text\": text,\n",
    "      \"ents\": ents,\n",
    "      \"title\": title,\n",
    "    }\n",
    "    ]\n",
    "    spacy.displacy.render(render_data, style=\"ent\", manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98a6902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Lütfen yardım \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Akevler mahallesi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">mahalle</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rüzgar sokak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">sokak</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuncay apartmanı\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Apartman/Site</span>\n",
       "</mark>\n",
       " zemin kat \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Antakya\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ilce</span>\n",
       "</mark>\n",
       " akrabalarım göçük altında #hatay #Afad</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"\"\"Lütfen yardım Akevler mahallesi Rüzgar sokak Tuncay apartmanı zemin kat Antakya akrabalarım göçük altında #hatay #Afad\"\"\"\n",
    "\n",
    "get_entities_html(sentence, nlp(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b823ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">@Cihatzn05 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fethiye mahallesi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">mahalle</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    geni sokak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">sokak</span>\n",
       "</mark>\n",
       "   \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Adıyaman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">il</span>\n",
       "</mark>\n",
       " / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tut\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ilce</span>\n",
       "</mark>\n",
       " 12 den beri ulaşamıyorum</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \" \".join(dataset[\"train\"][433][\"tokens\"])\n",
    "get_entities_html(sentence, nlp(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7003c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
